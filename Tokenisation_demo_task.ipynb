{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHfIIcFYRrT0ln8ox7V9b8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ahsan-folium/ai-intern-week04-deep-learnin/blob/main/Tokenisation_demo_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK**\n",
        "\n",
        "\"Tokenisation demo – inspect tokens, masks for bert-base-uncased.\n",
        "\n"
      ],
      "metadata": {
        "id": "pWTUDocmwjBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load BERT's tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n"
      ],
      "metadata": {
        "id": "cYOeUQUxxI5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"I am learning hugging face and its pretty good\""
      ],
      "metadata": {
        "id": "-L7TfmUXxUIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the sentence\n",
        "encoded = tokenizer(sentence)\n",
        "\n",
        "\n",
        "for key, value in encoded.items():\n",
        "    print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "id": "lSfkGUBJxdyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert IDs back to tokens\n",
        "tokens = tokenizer.convert_ids_to_tokens(encoded['input_ids'])\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "id": "uy56Tqlqxkfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NOTES\n",
        "\n",
        "bert-base-uncased = lowercase-only version of BERT (so “Hello” and “hello” are treated the same).\n",
        "\n",
        "\n",
        "Tokens\n",
        "\n",
        "[CLS] → special classification token BERT adds at start.\n",
        "\n",
        "[SEP] → special separator token added at end.\n",
        "\n",
        "hugging, face, is, amazing, ! → actual subword tokens.\n",
        "\n",
        "2. Input IDs\n",
        "\n",
        "Each token is mapped to a unique integer ID from BERT’s vocabulary (size = 30,522).\n",
        "\n",
        "Example:\n",
        "\n",
        "[CLS] → 101\n",
        "\n",
        "\"hugging\" → 17662\n",
        "\n",
        "\"face\" → 2227\n",
        "\n",
        "Tells BERT which tokens are real (1) and which are padding (0).\n",
        "\n",
        "Here: [1, 1, 1, 1, 1, 1, 1] because no padding is needed.\n",
        "\n",
        "\n",
        "Token Type IDs (a.k.a. Segment IDs)\n",
        "\n",
        "Used when BERT handles sentence pairs (e.g., Question + Answer).\n",
        "\n",
        "0 = first sentence, 1 = second sentence.\n",
        "\n",
        "Here all 0s, since we only gave 1 sentence."
      ],
      "metadata": {
        "id": "yeIgpV7FyrKP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wkaBM1_5y4kw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}